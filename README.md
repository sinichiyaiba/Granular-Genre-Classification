# Granular-Genre-Classification

Text genre classification has always been a subject of interest within the natural language processing field, due to the complexity of human languages and the nature of machine learning models in taking texts at face value. As such, standalone sentences and literary devices may confuse these models, as there may be hidden context or meaning that are not typically recognized. Despite the need for text genre classification models to be trained on a larger context window, time and computing resource constraints mean that most projects have historically relied on traditional models (such as support vector machines) and are trained on a very small amount of tokens per entry. However, with the arrival of transformer models, and powerful and openly available training platforms, the task seems more approachable than ever. Our project aims to classify creative literary works into the appropriate genres, and is different from previous attempts at this task in two meaningful ways: we provided our models of choice with much more context per input, and used the trained models to predict one or more genres per work. Our experiments showed promising results: using a variety of transformer-based models, we achieved a high accuracy of 81% using the DistilBERT model and 83.2% using the Longformer model for multi-label classification tasks.
